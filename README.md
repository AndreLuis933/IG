[![Python application](https://github.com/AndreLuis933/IG/actions/workflows/daily-request.yml/badge.svg)](https://github.com/AndreLuis933/IG/actions/workflows/daily-request.yml)
![Docker support](https://img.shields.io/badge/docker-supported-blue)
[![License](https://img.shields.io/github/license/AndreLuis933/IG)](LICENSE)

# üõí Irm√£os Gon√ßalves Scraper

Automatize o monitoramento de pre√ßos do maior supermercado de Rond√¥nia! Este projeto realiza scraping di√°rio do site [Irm√£os Gon√ßalves](https://www.irmaosgoncalves.com.br/), armazena os dados em banco de dados relacional e oferece visualiza√ß√£o interativa via Streamlit.

> üíª **Veja a visualiza√ß√£o online:**  
> [https://view-ig.streamlit.app/](https://view-ig.streamlit.app/)

---

## √çndice

1. [Funcionalidades](#funcionalidades)
2. [Visualiza√ß√£o dos Dados](#visualizacao-dos-dados)
3. [Estrutura do Projeto](#estrutura-do-projeto)
4. [Sobre a infraestrutura em nuvem](#sobre-a-infraestrutura-em-nuvem)
5. [Otimiza√ß√£o e volume de dados](#otimizacao-e-volume-de-dados)
6. [Estrutura do Banco de Dados](#estrutura-do-banco-de-dados)
7. [Pr√©-requisitos para rodar localmente](#pre-requisitos-para-rodar-localmente)
8. [Instala√ß√£o](#instalacao)
9. [Configura√ß√£o](#configuracao)
10. [Como Usar](#como-usar)
11. [Licen√ßa](#licenca)
12. [Contato](#contato)

---

## ‚ú® Funcionalidades

- üï∏Ô∏è Scraping automatizado do site do Irm√£os Gon√ßalves
- üì¶ Extra√ß√£o de nome, pre√ßo, categoria, imagem e link dos produtos
- ‚òÅÔ∏è Armazenamento dos dados em banco PostgreSQL
- üìä Visualiza√ß√£o da evolu√ß√£o de pre√ßos por categoria via Streamlit
- üê≥ Deploy simplificado com Docker e Fly.io
- üîÑ Automa√ß√£o di√°ria via GitHub Actions

---

## üìä Visualiza√ß√£o dos Dados

Veja abaixo exemplos de como os dados podem ser visualizados na aplica√ß√£o Streamlit:

![Dashboard Geral](docs/images/dashboard_geral.png)  
_Dashboard principal mostrando a evolu√ß√£o dos pre√ßos m√©dios ao longo do tempo._

![Filtro por Cidade](docs/images/Filtros.png)  
_Menu lateral com filtros din√¢micos para cidade, nome, pre√ßo, categoria e data._

![Gr√°fico por Categoria](docs/images/grafico_categoria.png)  
_Gr√°fico comparativo da evolu√ß√£o de pre√ßos por categoria de produto._

![Tabela de Dados](docs/images/tabela_dados.png)  
_Tabela interativa com todos os dados brutos, incluindo op√ß√£o de download em CSV._

---

## üìÅ Estrutura do Projeto

```plaintext
IRMAOS-GONCALVES-SCRAPER/
‚îú‚îÄ‚îÄ .github/
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ view/
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
```

- **scraper/**: Scripts de coleta e processamento de dados
- **view/**: Aplica√ß√£o Streamlit para visualiza√ß√£o
- **.github/**: Workflows de automa√ß√£o

---

## ‚òÅÔ∏è Sobre a infraestrutura em nuvem

Este projeto foi projetado para rodar diariamente de forma **totalmente automatizada e otimizada para custos** em ambiente de nuvem. Utilizamos [GitHub Actions](https://github.com/features/actions) para orquestra√ß√£o, [Fly.io](https://fly.io/) para hospedar os containers Docker do scraper (com desligamento autom√°tico da m√°quina ap√≥s a execu√ß√£o, o que permite que o projeto rode **sem custo financeiro dentro do plano gratuito**), [Supabase](https://supabase.com/) para armazenamento dos dados e [Streamlit Cloud](https://streamlit.io/cloud) para a visualiza√ß√£o online.

Essas configura√ß√µes permitem que o scraping, armazenamento e visualiza√ß√£o dos dados ocorram de forma aut√¥noma, eficiente e sem interven√ß√£o manual, garantindo a atualiza√ß√£o cont√≠nua dos dados com custo zero.

No entanto, toda a documenta√ß√£o deste README √© focada no uso local, para facilitar a replica√ß√£o e testes por qualquer pessoa. Se voc√™ tiver interesse em saber mais sobre a automa√ß√£o e deploy em nuvem, entre em contato.

---

## üìà Otimiza√ß√£o e volume de dados

Para garantir efici√™ncia e evitar custos desnecess√°rios no Supabase, o projeto adota estrat√©gias avan√ßadas de compress√£o e agrupamento dos dados hist√≥ricos. Em vez de registrar uma linha para cada dia, cada produto e cada cidade, os dados s√£o armazenados em intervalos cont√≠nuos de tempo e, quando poss√≠vel, agrupados por cidade. Isso reduz drasticamente o volume de registros, mantendo o hist√≥rico completo e detalhado para an√°lise.

O monitoramento est√° em opera√ß√£o desde **26/04/2025**, cobrindo atualmente **11 mercados** (lojas) e cerca de **18.000 produtos por mercado**. Gra√ßas a essas otimiza√ß√µes, mesmo com o crescimento cont√≠nuo dos dados, o projeto se mant√©m sustent√°vel e eficiente. Atualmente, a maior tabela (`disponibilidade_cidades`, que monitora a disponibilidade di√°ria de cada produto em cada loja) j√° conta com mais de **823 mil registros**.

> Para detalhes t√©cnicos sobre a estrutura das tabelas e regras de agrupamento, consulte a se√ß√£o [Estrutura do Banco de Dados](#estrutura-do-banco-de-dados).
<details>
<summary>üó∫Ô∏è Ver cidades e identifica√ß√£o das lojas monitoradas</summary>

**Cidades e lojas monitoradas:**

- Ariquemes
- Cacoal
- Jaru
- Ouro Preto do Oeste
- Rolim de Moura
- Vilhena

**Ji-Paran√°**
- Ji-Parana 1: DOIS DE ABRIL ‚Äî Av. Mal. Rondon c/ Rua dos Mineiros, n¬∫ 1793, CEP: 76.900-137
- Ji-Parana 2: CAFEZINHO ‚Äî Av. das Seringueiras, n¬∫ 1201, CEP: 76.913-112

**Porto Velho**
- Porto Velho 1: AV. SETE DE SETEMBRO (N.S. DAS GRA√áAS) ‚Äî CEP: 76.804-142
- Porto Velho 2: AV. AMAZONAS (TIRADENTES) ‚Äî CEP: 76.824-652
- Porto Velho 3: AVENIDA JATUARANA (CALADINHO) ‚Äî CEP: 76.808-110

</details>
---

## üóÑÔ∏è Estrutura do Banco de Dados

O projeto utiliza um banco relacional com as seguintes tabelas principais:

<details>
<summary>Clique para ver o diagrama Mermaid</summary>

```mermaid
erDiagram
    PRODUTOS ||--o| IMAGENS : tem
    PRODUTOS ||--o{ HISTORICO_PRECOS : possui
    PRODUTOS ||--o{ DISPONIBILIDADE_CIDADES : possui

    CIDADES ||--o{ HISTORICO_PRECOS : possui
    CIDADES ||--o{ DISPONIBILIDADE_CIDADES : possui

    PRODUTOS {
        int id PK
        string nome
        string link
        string categoria
        date data_atualizacao
    }

    IMAGENS {
        int produto_id PK,FK
        string link_imagem
        date data_atualizacao
    }

    CIDADES {
        int id PK
        string nome
    }

    HISTORICO_PRECOS {
        int id PK
        int produto_id FK
        int cidade_id FK
        float preco
        date data_inicio
        date data_fim
        unique(produto_id, cidade_id, data_inicio)
    }

    DISPONIBILIDADE_CIDADES {
        int id PK
        int produto_id FK
        int cidade_id FK
        bool disponivel
        date data_inicio
        date data_fim
        unique(produto_id, cidade_id, data_inicio)
    }

    LOG_EXECUCAO {
        date data_execucao PK
    }
```

</details>

**Observa√ß√µes de Implementa√ß√£o:**

- As datas de in√≠cio e fim (`data_inicio`, `data_fim`) s√£o inclusivas.
- As tabelas s√£o criadas automaticamente na primeira execu√ß√£o do scraper, n√£o √© necess√°rio rodar scripts SQL manualmente.
- Na tabela `historico_precos`, s√≥ √© salvo um registro para todas as cidades se o pre√ßo for igual em todas; caso contr√°rio, √© salvo um pre√ßo espec√≠fico para cada cidade.
- Se o pre√ßo for o mesmo para todas as cidades dispon√≠veis naquele dia, o `id` da cidade ser√° `1`.

---

## üõ†Ô∏è Pr√©-requisitos para rodar localmente

- Python 3.12+
- PostgreSQL 15+
- Navegador web compat√≠vel com Selenium (ex: Chrome, Firefox)

---

## üöÄ Instala√ß√£o

```bash
git clone https://github.com/AndreLuis933/IG.git
cd IG
```

---

## ‚öôÔ∏è Configura√ß√£o

<details>
<summary>Vari√°veis de ambiente</summary>

1. Crie um banco de dados PostgreSQL local e anote as credenciais.
2. Crie um arquivo `.env` na raiz do projeto com o seguinte conte√∫do (ajuste para suas credenciais):

```env
DATABASE_URL=postgresql+psycopg2://meuusuario:minhasenha@localhost:5432/minhabasededados
LOCAL=true
```

- N√£o √© necess√°rio configurar cookies manualmente, a coleta √© autom√°tica.
- As tabelas do banco s√£o criadas automaticamente na primeira execu√ß√£o do scraper.
</details>

---

## ‚ñ∂Ô∏è Como Usar

Abra **dois terminais**:

- **Terminal 1: Rodando o scraper**

  ```bash
  cd scraper
  python -m venv .venv
  pip install -r requirements.txt
  python main.py
  ```

  O scraper ir√° coletar os dados e salvar no banco de dados local.

- **Terminal 2: Visualiza√ß√£o dos dados**

  ```bash
  cd view
  python -m venv .venv
  pip install -r requirements.txt
  streamlit run Dashboard.py
  ```

  O Streamlit ir√° buscar os dados automaticamente do banco e exibir a interface interativa.

- **Download dos dados:**  
  O download dos dados brutos em CSV est√° dispon√≠vel diretamente na interface do Streamlit.

---

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## üôã‚Äç‚ôÇÔ∏è Contato

Fique √† vontade para abrir uma issue, sugerir melhorias ou s√≥ bater um papo sobre dados, scraping e automa√ß√£o!

- **LinkedIn:** [linkedin.com/in/andreluissouzacardoso](https://www.linkedin.com/in/andreluissouzacardoso/)  
- **GitHub:** [github.com/AndreLuis933](https://github.com/AndreLuis933)

Se quiser saber mais sobre o projeto, contribuir ou trocar ideias, √© s√≥ chamar!
